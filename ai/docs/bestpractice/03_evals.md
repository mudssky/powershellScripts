# 评估与测试 (Evals) —— 最难但也最重要的一环

传统单元测试（`assert a == b`）对 LLM 无效，因为它的回答每次都不一样。

**1. 建立“黄金数据集” (Golden Dataset)**

* **实践**：在开发初期，手动准备 50-100 个典型的高质量“输入-输出”对。这是你系统上线前的底线。

**2. LLM-as-a-Judge (用 LLM 当裁判)**

* **原理**：让一个更强的模型（如 GPT-4）去给你的应用模型（如 GPT-3.5）的回答打分。
* **维度**：不要只打一个分。要分维度：准确性、相关性、有害性、格式合规性。
* **工具**：**Ragas** (专门评测 RAG)、**DeepEval**。

**3. 确定性测试与概率性测试分离**

* **代码逻辑**（Tool 调用是否正确、JSON 解析是否报错）必须过传统的 CI/CD 单元测试。
* **回答质量**（语气是否友好、内容是否正确）通过离线批量跑 Evaluation Pipeline 监控。
